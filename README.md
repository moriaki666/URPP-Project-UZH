# URPP-Project-UZH
Jupyter Notebooks created while working as a Research Assistant for the URPP project at the University of Zurich

Task 1: 5 notebooks
 Task 1: Source Material : A collection of 400 letters written in old Dutch hosted on an academic
website in the form of HTML and XML files. 
- 1.1 Extract all possible information from 2000 HTML and XML files related to the 400 letters
and create a dataset. (Python and R)
- 1.2 Perform data analysis on the dataset (Python)
- 1.3 Use Machine Translation to convert the main content extracted from the letters to English.
Train an LLM model fine-tuned for this task and compare the translations (Python and GPT-4) 
- 1.4 Perform Named Entity Extraction, and Sentiment Analysis on the translated content
- 1.5 Perform further experiments and analysis based on the results of 1.2, 1.3 and 1.4

Task 2: 4 notebooks
Task 2: Source Material: An ebook containing hundreds of letters exchanged between nuns in a
convent. The letters are in old German and written between the years 1400-1600.
- 2.1 Extract relevant information from the pdf file of the ebook and create a dataset mostly
containing information about selected topics and also letters containing information about
certain items exchanged that are relevant to the goal of the project. (Python and R)
- 2.2 Perform data analysis on the dataset (Python)
- 2.3 Use Machine Translation to convert the main content extracted from the letters to English.
Train an LLM model fine-tuned for this task and compare the translations (Python and GPT-4)
- 2.4 Perform Named Entity Extraction, and Sentiment Analysis on the translated content
- 2..5 Perform further experiments and analysis based on the results of 2.2, 2.3 and 2.4

LLM fine-tuning code cannot be uploaded due to privacy reasons
The NER_spaCy_BERT.ipynb is used for both tasks 1 and 2


